
## 1. 导入tf和keras
首先检验一下安装的成果吧，导入tensorflow并查看版本（注意kernel的选择）


```python
import tensorflow as tf
import tensorflow.keras
from tensorflow.keras import layers
print(tf.__version__)
print(tf.keras.__version__)
```

    2.0.0-beta1
    2.2.4-tf
    

如果没有报错的话，那恭喜大家成功迈出了第一步啦~
tensorflow2中集成了keras，其中keras.layers中囊括了常见的神经网络结构，那么下面我们就开始使用tf.keras进行简单模型的构建吧。
## 2. 构建简单模型
### 2.1 模型堆叠
首先构建一个序列堆叠的网络模型，使用`tf.keras.Sequential`结构初始化`model`，并且通过`model.add`一层一层堆叠，堆叠的对象是在keras.layers类中预先定义的全连接层（Dense）：


```python
model = tf.keras.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(72,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
```

有经验的同学应该可以发现这是一个三层的全连接神经网络：使用`input_shape`进行网络输入的格式规定，前两层的激活函数为`relu`，最后一层使用了一个10维的`softmax`进行多分类的归一化。使用`tf.keras.Sequential`序列模型好像搭积木一样，不断用`model.add`拼上去，就构建完成了我们的第一个网络结构。
### 2.2 layer参数配置
大家一定跃跃欲试了吧，不过在测试咱们的网络之前，还需要了解一下`tf.keras.layers`中的一系列参数：
- `activation`：顾名思义就是该层的激活函数啦。此参数可以通过内置函数的名称指定，或者指定为可调用对象。默认情况下，系统不会应用任何激活函数。常用的激活函数有：
    - linear
    - sigmoid
    - hard_sigmoid
    - relu
    - tanh
    - softmax
    - softsigh
    - softplus
- `kernel_initializer` 和 `bias_initializer`：创建层权重（核和偏差）的初始化方法。此参数是一个名称或可调用对象，默认为 "Glorot uniform" 初始化器。常用的初始化方法有：
    - zero（不推荐）
    - uniform
    - lecun_uniform: 即有输入节点数之平方根放缩后的均匀分布初始化（LeCun 98）.
    - normal
    - identity：仅用于权值矩阵为方阵的2D层（shape[0]=shape[1]）
    - orthogonal：仅用于权值矩阵为方阵的2D层（shape[0]=shape[1]），参考Saxe et al.
    - glorot_normal：由扇入扇出放缩后的高斯初始化（Glorot 2010）
    - glorot_uniform
    - he_normal：由扇入放缩后的高斯初始化（He et al.,2014）
    - he_uniform
- `kernel_regularizer` 和 `bias_regularizer`：应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。常用的正则化函数如下（使用`keras.regularizers`调用）：
    - l1(0.01)：L1正则项，又称LASSO
    - l2(0.01)：L2正则项，又称权重衰减或Ridge
    - l1_l2(l1=0.01, l2=0.01)： L1-L2混合正则项, 又称ElasticNet

现在大家可以试一下各种激活函数、初始化方案和正则函数的组合~


```python
layers.Dense(32, activation='sigmoid')
layers.Dense(32, activation=tf.sigmoid)
layers.Dense(32, kernel_initializer='orthogonal')
layers.Dense(32, kernel_initializer=tf.keras.initializers.glorot_normal)
layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))
layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))
```




    <tensorflow.python.keras.layers.core.Dense at 0x22c5c2d4788>



## 3. 训练和评估
### 3.1 设置训练流程
现在我们有了堆叠好的模型，在正式开始训练模型之前，还需要使用`model.compile`配置一下训练流程：


```python
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),
             loss=tf.keras.losses.categorical_crossentropy,
             metrics=[tf.keras.metrics.categorical_crossentropy])
```

### 3.2 compile参数配置
依然还是先了解一下参数：
- `optimizer`：可以在调用model.compile()之前初始化一个优化器对象，然后传入该函数（如上所示），也可以在调用model.compile()时传递一个预定义优化器名。所有优化器可以在括号中输入`lr`参数，也可以输入`clipnorm`和`clipvalue`进行梯度裁剪。预定义的优化器有如下几种（参数可选）：
    - SGD：`keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)`
    - RMSprop：`keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)`
    - Adagrad：`keras.optimizers.Adagrad(lr=0.01, epsilon=1e-06)`
    - Adadelta：`keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)`
    - Adam：`keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)`
    - Adamax：`keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)`
    - Nadam：`keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)`
- `loss`：指定模型的误差，即目标函数，可用的目标函数如下所示：
    - mean_squared_error或mse
    - mean_absolute_error或mae
    - mean_absolute_percentage_error或mape
    - mean_squared_logarithmic_error或msle
    - squared_hinge
    - hinge
    - binary_crossentropy（亦称作对数损失，logloss）
    - categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列
    - sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)
    - kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.
    - poisson：即(predictions - targets * log(predictions))的均值
    - cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数
- `metrics`：与loss类似，不过此时的函数只用于模型性能评估但是不会用于训练，多和loss使用一样的函数。

### 3.2 随机生成数据进行训练
上一步我们已经配置好了模型的训练流程，现在随机生成一些数据进行模型训练：
#### 3.2.1 使用numpy输入数据并训练


```python
import numpy as np

train_x = np.random.random((1000, 72))
train_y = np.random.random((1000, 10))

val_x = np.random.random((200, 72))
val_y = np.random.random((200, 10))

model.fit(train_x, train_y, epochs=10, batch_size=100,
          validation_data=(val_x, val_y))
```

    Train on 1000 samples, validate on 200 samples
    Epoch 1/10
    1000/1000 [==============================] - 0s 41us/sample - loss: 11.6317 - categorical_crossentropy: 11.6317 - val_loss: 11.4970 - val_categorical_crossentropy: 11.4970
    Epoch 2/10
    1000/1000 [==============================] - 0s 40us/sample - loss: 11.6301 - categorical_crossentropy: 11.6301 - val_loss: 11.4983 - val_categorical_crossentropy: 11.4983
    Epoch 3/10
    1000/1000 [==============================] - 0s 43us/sample - loss: 11.6295 - categorical_crossentropy: 11.6295 - val_loss: 11.4974 - val_categorical_crossentropy: 11.4974
    Epoch 4/10
    1000/1000 [==============================] - 0s 46us/sample - loss: 11.6286 - categorical_crossentropy: 11.6286 - val_loss: 11.4966 - val_categorical_crossentropy: 11.4966
    Epoch 5/10
    1000/1000 [==============================] - 0s 36us/sample - loss: 11.6283 - categorical_crossentropy: 11.6283 - val_loss: 11.4990 - val_categorical_crossentropy: 11.4990
    Epoch 6/10
    1000/1000 [==============================] - 0s 37us/sample - loss: 11.6267 - categorical_crossentropy: 11.6267 - val_loss: 11.4996 - val_categorical_crossentropy: 11.4996
    Epoch 7/10
    1000/1000 [==============================] - 0s 36us/sample - loss: 11.6275 - categorical_crossentropy: 11.6275 - val_loss: 11.4990 - val_categorical_crossentropy: 11.4990
    Epoch 8/10
    1000/1000 [==============================] - 0s 36us/sample - loss: 11.6275 - categorical_crossentropy: 11.6275 - val_loss: 11.4961 - val_categorical_crossentropy: 11.4961
    Epoch 9/10
    1000/1000 [==============================] - 0s 38us/sample - loss: 11.6269 - categorical_crossentropy: 11.6269 - val_loss: 11.5009 - val_categorical_crossentropy: 11.5009
    Epoch 10/10
    1000/1000 [==============================] - 0s 40us/sample - loss: 11.6267 - categorical_crossentropy: 11.6267 - val_loss: 11.4958 - val_categorical_crossentropy: 11.4958
    




    <tensorflow.python.keras.callbacks.History at 0x22c8bd4bac8>



大家的模型是否都跑起来了呢？如果成功的话，可以看到每一个epoch训练的时间、训练集的评价和验证集的评价。

细心的同学可能已经发现了，此时随机生成的标签y其实上是无意义的，应该是onehot表示才对。不过此时我们只是初步试验一下模型，所以只要维度正确模型的含义可以先不管~
#### 3.2.2 使用tf.data输入数据并训练
除了使用numpy输入，也可以通过`tf.data`将训练和验证集拼接后输入：


```python
dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))
dataset = dataset.batch(32)
dataset = dataset.repeat()
val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))
val_dataset = val_dataset.batch(32)
val_dataset = val_dataset.repeat()

model.fit(dataset, epochs=10, steps_per_epoch=30,
          validation_data=val_dataset, validation_steps=3, shuffle = True)
```

    W0814 17:40:34.343302 13260 training_utils.py:1436] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.
    

    Epoch 1/10
    30/30 [==============================] - 0s 7ms/step - loss: 11.6263 - categorical_crossentropy: 11.6263 - val_loss: 11.5856 - val_categorical_crossentropy: 11.5856
    Epoch 2/10
    30/30 [==============================] - 0s 6ms/step - loss: 11.6413 - categorical_crossentropy: 11.6348 - val_loss: 11.5869 - val_categorical_crossentropy: 11.5869
    Epoch 3/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6633 - categorical_crossentropy: 11.6575 - val_loss: 11.5846 - val_categorical_crossentropy: 11.5846
    Epoch 4/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6218 - categorical_crossentropy: 11.6153 - val_loss: 11.5844 - val_categorical_crossentropy: 11.5844
    Epoch 5/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6205 - categorical_crossentropy: 11.6141 - val_loss: 11.5836 - val_categorical_crossentropy: 11.5836
    Epoch 6/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6288 - categorical_crossentropy: 11.6225 - val_loss: 11.5840 - val_categorical_crossentropy: 11.5840
    Epoch 7/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6389 - categorical_crossentropy: 11.6329 - val_loss: 11.5826 - val_categorical_crossentropy: 11.5826
    Epoch 8/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6480 - categorical_crossentropy: 11.6425 - val_loss: 11.5826 - val_categorical_crossentropy: 11.5826
    Epoch 9/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6312 - categorical_crossentropy: 11.6255 - val_loss: 11.5861 - val_categorical_crossentropy: 11.5861
    Epoch 10/10
    30/30 [==============================] - 0s 4ms/step - loss: 11.6464 - categorical_crossentropy: 11.6412 - val_loss: 11.5862 - val_categorical_crossentropy: 11.5862
    




    <tensorflow.python.keras.callbacks.History at 0x22c8bf1bb88>



### 3.3 `model.fit`参数配置
在跑通了第一个模型之后，大家平复一下激动的心情，来看一下训练中的参数：
- X：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array
- y：标签，numpy array
- batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。
- epochs：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为"number of"的意思
- steps_per_epoch：整数，含义为每个epoch分割为多少个batch_size
- validation_data：形式为（X，y）的tuple，是指定的验证集。
- shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。

### 3.4 评估与预测
现在到了检验模型效果的时候了，可以使用`model.evaluate`和`model.predict`函数进行模型的评价和预测：


```python
test_x = np.random.random((1000, 72))
test_y = np.random.random((1000, 10))
model.evaluate(test_x, test_y, batch_size=32)
test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y))
test_data = test_data.batch(32).repeat()
model.evaluate(test_data, steps=30)
# predict
result = model.predict(test_x, batch_size=32)
print(result)
```

    1000/1000 [==============================] - 0s 87us/sample - loss: 11.4095 - categorical_crossentropy: 11.4095
    30/30 [==============================] - 0s 2ms/step - loss: 11.4043 - categorical_crossentropy: 11.4043
    [[0.1007135  0.10093972 0.09766549 ... 0.09846033 0.09654344 0.10093378]
     [0.09626372 0.10397251 0.09768865 ... 0.09703124 0.09973648 0.10044623]
     [0.10291155 0.09354296 0.09900691 ... 0.1022433  0.09772544 0.10057465]
     ...
     [0.10291155 0.09354296 0.09900691 ... 0.1022433  0.09772544 0.10057465]
     [0.09878294 0.10250331 0.09760903 ... 0.09784885 0.09796241 0.10057867]
     [0.09367913 0.1048402  0.09807841 ... 0.09614412 0.10175334 0.10068818]]
    

此处的batch_size为一次传入的数据数目。如果硬件允许的话，数值越大速度越快。

## 4 保存和恢复
### 4.1 权重的保存和读取
可以通过`model.save_weights`和`model.load_weights`来进行权重的保存和读取，参数为文件路径


```python
model.save_weights('../weights/weights')
model.load_weights('../weights/weights')
model.save_weights('../weights/weights.h5')
model.load_weights('../weights/weights.h5')
```

### 4.2 保存网络结构


```python
# 序列化成json
import json
import pprint
json_str = model.to_json()
pprint.pprint(json.loads(json_str))
fresh_model = tf.keras.models.model_from_json(json_str)
# 保持为yaml格式  #需要提前安装pyyaml

yaml_str = model.to_yaml()
print(yaml_str)
fresh_model = tf.keras.models.model_from_yaml(yaml_str)
```

    {'backend': 'tensorflow',
     'class_name': 'Sequential',
     'config': {'layers': [{'class_name': 'Dense',
                            'config': {'activation': 'relu',
                                       'activity_regularizer': None,
                                       'batch_input_shape': [None, 72],
                                       'bias_constraint': None,
                                       'bias_initializer': {'class_name': 'Zeros',
                                                            'config': {}},
                                       'bias_regularizer': None,
                                       'dtype': 'float32',
                                       'kernel_constraint': None,
                                       'kernel_initializer': {'class_name': 'GlorotUniform',
                                                              'config': {'seed': None}},
                                       'kernel_regularizer': None,
                                       'name': 'dense',
                                       'trainable': True,
                                       'units': 32,
                                       'use_bias': True}},
                           {'class_name': 'Dense',
                            'config': {'activation': 'relu',
                                       'activity_regularizer': None,
                                       'bias_constraint': None,
                                       'bias_initializer': {'class_name': 'Zeros',
                                                            'config': {}},
                                       'bias_regularizer': None,
                                       'dtype': 'float32',
                                       'kernel_constraint': None,
                                       'kernel_initializer': {'class_name': 'GlorotUniform',
                                                              'config': {'seed': None}},
                                       'kernel_regularizer': None,
                                       'name': 'dense_1',
                                       'trainable': True,
                                       'units': 64,
                                       'use_bias': True}},
                           {'class_name': 'Dense',
                            'config': {'activation': 'softmax',
                                       'activity_regularizer': None,
                                       'bias_constraint': None,
                                       'bias_initializer': {'class_name': 'Zeros',
                                                            'config': {}},
                                       'bias_regularizer': None,
                                       'dtype': 'float32',
                                       'kernel_constraint': None,
                                       'kernel_initializer': {'class_name': 'GlorotUniform',
                                                              'config': {'seed': None}},
                                       'kernel_regularizer': None,
                                       'name': 'dense_2',
                                       'trainable': True,
                                       'units': 10,
                                       'use_bias': True}}],
                'name': 'sequential'},
     'keras_version': '2.2.4-tf'}
    backend: tensorflow
    class_name: Sequential
    config:
      layers:
      - class_name: Dense
        config:
          activation: relu
          activity_regularizer: null
          batch_input_shape: !!python/tuple
          - null
          - 72
          bias_constraint: null
          bias_initializer:
            class_name: Zeros
            config: {}
          bias_regularizer: null
          dtype: float32
          kernel_constraint: null
          kernel_initializer:
            class_name: GlorotUniform
            config:
              seed: null
          kernel_regularizer: null
          name: dense
          trainable: true
          units: 32
          use_bias: true
      - class_name: Dense
        config:
          activation: relu
          activity_regularizer: null
          bias_constraint: null
          bias_initializer:
            class_name: Zeros
            config: {}
          bias_regularizer: null
          dtype: float32
          kernel_constraint: null
          kernel_initializer:
            class_name: GlorotUniform
            config:
              seed: null
          kernel_regularizer: null
          name: dense_1
          trainable: true
          units: 64
          use_bias: true
      - class_name: Dense
        config:
          activation: softmax
          activity_regularizer: null
          bias_constraint: null
          bias_initializer:
            class_name: Zeros
            config: {}
          bias_regularizer: null
          dtype: float32
          kernel_constraint: null
          kernel_initializer:
            class_name: GlorotUniform
            config:
              seed: null
          kernel_regularizer: null
          name: dense_2
          trainable: true
          units: 10
          use_bias: true
      name: sequential
    keras_version: 2.2.4-tf
    
    

    D:\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\saving\model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
      config = yaml.load(yaml_string)
    

### 4.3 保存整个模型
可以通过`model.save`和`tf.keras.models.load_model`命令保存和读取整个模型：


```python
model.save('../models/all_model.h5')
model = tf.keras.models.load_model('../models/all_model.h5')
```

到这里我们就完成了全部第一课的学习啦，现在我们已经可以通过`keras.layers`内置的全连接层，搭建我们自己的网络结构啦~大家可以可以找一找身边数据集，have a try！
