
## Test1. 实现mnist手写字体识别（全连接神经网络）
上节课我们已经实现了使用tf中的keras构建简单网络，现在我们尝试用简单的全连接网络来解决经典的mnist手写字体识别问题：
### 1.1 导入tf、keras和keras.layers


```python
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
import numpy as np
```

### 1.2 导入并处理数据


```python
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
print(type(x_train), len(x_train))
print(type(y_test), len(y_test))
print(type(x_train[0]), np.shape(x_train[0]))
print(type(y_train[0]))
```

    <class 'numpy.ndarray'> 60000
    <class 'numpy.ndarray'> 10000
    <class 'numpy.ndarray'> (28, 28)
    <class 'numpy.uint8'>
    

可以看出训练集有60000个样本，测试集有10000个，每个x有28\*28维特征，y为一维的字体分类。

接下来有两个预处理步骤：
- 因为使用全连接网络，可以将接下来我们要将x直接拼接为1\*784维度，并且统一转化为tensorflow常用的float32变量；
- x中的值为图像的灰度表示（0~255），可以做一个归一化。(会对结果有影响吗？)




```python
x_train = x_train.reshape(60000, 784).astype('float32')/255
x_test = x_test.reshape(10000, 784).astype('float32')/255
print(x_train[1])
```

    [0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.2        0.62352943 0.99215686 0.62352943 0.19607843
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.1882353  0.93333334
     0.9882353  0.9882353  0.9882353  0.92941177 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.21176471 0.8901961  0.99215686 0.9882353  0.9372549
     0.9137255  0.9882353  0.22352941 0.02352941 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.03921569 0.23529412 0.8784314
     0.9882353  0.99215686 0.9882353  0.7921569  0.32941177 0.9882353
     0.99215686 0.47843137 0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.6392157  0.9882353  0.9882353  0.9882353  0.99215686
     0.9882353  0.9882353  0.3764706  0.7411765  0.99215686 0.654902
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.2        0.93333334
     0.99215686 0.99215686 0.74509805 0.44705883 0.99215686 0.89411765
     0.18431373 0.30980393 1.         0.65882355 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.1882353  0.93333334 0.9882353  0.9882353  0.7019608
     0.04705882 0.29411766 0.4745098  0.08235294 0.         0.
     0.99215686 0.9529412  0.19607843 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.14901961 0.64705884
     0.99215686 0.9137255  0.8156863  0.32941177 0.         0.
     0.         0.         0.         0.         0.99215686 0.9882353
     0.64705884 0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.02745098 0.69803923 0.9882353  0.9411765  0.2784314
     0.07450981 0.10980392 0.         0.         0.         0.
     0.         0.         0.99215686 0.9882353  0.7647059  0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.22352941
     0.9882353  0.9882353  0.24705882 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.99215686 0.9882353  0.7647059  0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.7764706  0.99215686 0.74509805
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         1.         0.99215686
     0.76862746 0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.29803923 0.9647059  0.9882353  0.4392157  0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.99215686 0.9882353  0.5803922  0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.33333334 0.9882353
     0.9019608  0.09803922 0.         0.         0.         0.
     0.         0.         0.         0.         0.02745098 0.5294118
     0.99215686 0.7294118  0.04705882 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.33333334 0.9882353  0.8745098  0.
     0.         0.         0.         0.         0.         0.
     0.         0.02745098 0.5137255  0.9882353  0.88235295 0.2784314
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.33333334 0.9882353  0.5686275  0.         0.         0.
     0.         0.         0.         0.         0.1882353  0.64705884
     0.9882353  0.6784314  0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.3372549  0.99215686
     0.88235295 0.         0.         0.         0.         0.
     0.         0.44705883 0.93333334 0.99215686 0.63529414 0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.33333334 0.9882353  0.9764706  0.57254905
     0.1882353  0.11372549 0.33333334 0.69803923 0.88235295 0.99215686
     0.8745098  0.654902   0.21960784 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.33333334 0.9882353  0.9882353  0.9882353  0.8980392  0.84313726
     0.9882353  0.9882353  0.9882353  0.76862746 0.50980395 0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.10980392 0.78039217
     0.9882353  0.9882353  0.99215686 0.9882353  0.9882353  0.9137255
     0.5686275  0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.09803922 0.5019608  0.9882353
     0.99215686 0.9882353  0.5529412  0.14509805 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.        ]
    

### 1.3 网络搭建
现在我们开始搭建我们的模型：
1. 784维的输入
2. 32个神经元的全连接层
3. 64个神经元的全连接层
4. 10个神经元的softmax输出层


```python
model = keras.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
```


```python
model.compile(optimizer=keras.optimizers.Adam(0.005),
             loss = keras.losses.sparse_categorical_crossentropy,
             metrics = [keras.losses.sparse_categorical_crossentropy])
```

### 1.4 网络训练


```python
model.fit(x_train, y_train, epochs=10, batch_size=100, 
         validation_split=0.2)
```

    Train on 48000 samples, validate on 12000 samples
    Epoch 1/10
    48000/48000 [==============================] - 3s 72us/sample - loss: 0.0465 - sparse_categorical_crossentropy: 0.0465 - val_loss: 0.2374 - val_sparse_categorical_crossentropy: 0.2374
    Epoch 2/10
    48000/48000 [==============================] - 3s 61us/sample - loss: 0.0326 - sparse_categorical_crossentropy: 0.0326 - val_loss: 0.2570 - val_sparse_categorical_crossentropy: 0.2570
    Epoch 3/10
    48000/48000 [==============================] - 3s 61us/sample - loss: 0.0292 - sparse_categorical_crossentropy: 0.0292 - val_loss: 0.2482 - val_sparse_categorical_crossentropy: 0.2482
    Epoch 4/10
    48000/48000 [==============================] - 3s 62us/sample - loss: 0.0342 - sparse_categorical_crossentropy: 0.0342 - val_loss: 0.2601 - val_sparse_categorical_crossentropy: 0.2601
    Epoch 5/10
    48000/48000 [==============================] - 3s 61us/sample - loss: 0.0318 - sparse_categorical_crossentropy: 0.0318 - val_loss: 0.2652 - val_sparse_categorical_crossentropy: 0.2652
    Epoch 6/10
    48000/48000 [==============================] - 3s 62us/sample - loss: 0.0273 - sparse_categorical_crossentropy: 0.0273 - val_loss: 0.2623 - val_sparse_categorical_crossentropy: 0.2623
    Epoch 7/10
    48000/48000 [==============================] - 3s 61us/sample - loss: 0.0278 - sparse_categorical_crossentropy: 0.0278 - val_loss: 0.2961 - val_sparse_categorical_crossentropy: 0.2961
    Epoch 8/10
    48000/48000 [==============================] - 3s 60us/sample - loss: 0.0238 - sparse_categorical_crossentropy: 0.0238 - val_loss: 0.2831 - val_sparse_categorical_crossentropy: 0.2831
    Epoch 9/10
    48000/48000 [==============================] - 3s 59us/sample - loss: 0.0244 - sparse_categorical_crossentropy: 0.0244 - val_loss: 0.2696 - val_sparse_categorical_crossentropy: 0.2696
    Epoch 10/10
    48000/48000 [==============================] - 3s 59us/sample - loss: 0.0282 - sparse_categorical_crossentropy: 0.0282 - val_loss: 0.2759 - val_sparse_categorical_crossentropy: 0.2759
    




    <tensorflow.python.keras.callbacks.History at 0x1cfafabb1c8>



### 1.5 在验证集上测试


```python
model.evaluate(x_test, y_test, batch_size=32)
```

    10000/10000 [==============================] - 1s 106us/sample - loss: 0.2632 - sparse_categorical_crossentropy: 0.2632
    




    [0.2631902001183575, 0.26319027]



现在就完成了简单的全连接神经网络对mnist手写字体识别的分类问题，大家可以根据自己爱好调整策略和超参数，开始成为调参侠的第一步啦~
