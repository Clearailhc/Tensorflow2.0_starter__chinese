{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入tf和keras\n",
    "首先检验一下安装的成果吧，导入tensorflow并查看版本（注意kernel的选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有报错的话，那恭喜大家成功迈出了第一步啦~\n",
    "tensorflow2中集成了keras，其中keras.layers中囊括了常见的神经网络结构，那么下面我们就开始使用tf.keras进行简单模型的构建吧。\n",
    "## 2. 构建简单模型\n",
    "### 2.1 模型堆叠\n",
    "首先构建一个序列堆叠的网络模型，使用`tf.keras.Sequential`结构初始化`model`，并且通过`model.add`一层一层堆叠，堆叠的对象是在keras.layers类中预先定义的全连接层（Dense）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(72,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有经验的同学应该可以发现这是一个三层的全连接神经网络：使用`input_shape`进行网络输入的格式规定，前两层的激活函数为`relu`，最后一层使用了一个10维的`softmax`进行多分类的归一化。使用`tf.keras.Sequential`序列模型好像搭积木一样，不断用`model.add`拼上去，就构建完成了我们的第一个网络结构。\n",
    "### 2.2 layer参数配置\n",
    "大家一定跃跃欲试了吧，不过在测试咱们的网络之前，还需要了解一下`tf.keras.layers`中的一系列参数：\n",
    "- `activation`：顾名思义就是该层的激活函数啦。此参数可以通过内置函数的名称指定，或者指定为可调用对象。默认情况下，系统不会应用任何激活函数。常用的激活函数有：\n",
    "    - linear\n",
    "    - sigmoid\n",
    "    - hard_sigmoid\n",
    "    - relu\n",
    "    - tanh\n",
    "    - softmax\n",
    "    - softsigh\n",
    "    - softplus\n",
    "- `kernel_initializer` 和 `bias_initializer`：创建层权重（核和偏差）的初始化方法。此参数是一个名称或可调用对象，默认为 \"Glorot uniform\" 初始化器。常用的初始化方法有：\n",
    "    - zero（不推荐）\n",
    "    - uniform\n",
    "    - lecun_uniform: 即有输入节点数之平方根放缩后的均匀分布初始化（LeCun 98）.\n",
    "    - normal\n",
    "    - identity：仅用于权值矩阵为方阵的2D层（shape[0]=shape[1]）\n",
    "    - orthogonal：仅用于权值矩阵为方阵的2D层（shape[0]=shape[1]），参考Saxe et al.\n",
    "    - glorot_normal：由扇入扇出放缩后的高斯初始化（Glorot 2010）\n",
    "    - glorot_uniform\n",
    "    - he_normal：由扇入放缩后的高斯初始化（He et al.,2014）\n",
    "    - he_uniform\n",
    "- `kernel_regularizer` 和 `bias_regularizer`：应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。常用的正则化函数如下（使用`keras.regularizers`调用）：\n",
    "    - l1(0.01)：L1正则项，又称LASSO\n",
    "    - l2(0.01)：L2正则项，又称权重衰减或Ridge\n",
    "    - l1_l2(l1=0.01, l2=0.01)： L1-L2混合正则项, 又称ElasticNet\n",
    "\n",
    "现在大家可以试一下各种激活函数、初始化方案和正则函数的组合~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x26a77938488>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Dense(32, activation='sigmoid')\n",
    "layers.Dense(32, activation=tf.sigmoid)\n",
    "layers.Dense(32, kernel_initializer='orthogonal')\n",
    "layers.Dense(32, kernel_initializer=tf.keras.initializers.glorot_normal)\n",
    "layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练和评估\n",
    "### 3.1 设置训练流程\n",
    "现在我们有了堆叠好的模型，在正式开始训练模型之前，还需要使用`model.compile`配置一下训练流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "             loss=tf.keras.losses.categorical_crossentropy,\n",
    "             metrics=[tf.keras.metrics.categorical_crossentropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 compile参数配置\n",
    "依然还是先了解一下参数：\n",
    "- `optimizer`：可以在调用model.compile()之前初始化一个优化器对象，然后传入该函数（如上所示），也可以在调用model.compile()时传递一个预定义优化器名。所有优化器可以在括号中输入`lr`参数，也可以输入`clipnorm`和`clipvalue`进行梯度裁剪。预定义的优化器有如下几种（参数可选）：\n",
    "    - SGD：`keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)`\n",
    "    - RMSprop：`keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)`\n",
    "    - Adagrad：`keras.optimizers.Adagrad(lr=0.01, epsilon=1e-06)`\n",
    "    - Adadelta：`keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)`\n",
    "    - Adam：`keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)`\n",
    "    - Adamax：`keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)`\n",
    "    - Nadam：`keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)`\n",
    "- `loss`：指定模型的误差，即目标函数，可用的目标函数如下所示：\n",
    "    - mean_squared_error或mse\n",
    "    - mean_absolute_error或mae\n",
    "    - mean_absolute_percentage_error或mape\n",
    "    - mean_squared_logarithmic_error或msle\n",
    "    - squared_hinge\n",
    "    - hinge\n",
    "    - binary_crossentropy（亦称作对数损失，logloss）\n",
    "    - categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列\n",
    "    - sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)\n",
    "    - kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.\n",
    "    - poisson：即(predictions - targets * log(predictions))的均值\n",
    "    - cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数\n",
    "- `metrics`：与loss类似，不过此时的函数只用于模型性能评估但是不会用于训练，多和loss使用一样的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 随机生成数据进行训练\n",
    "上一步我们已经配置好了模型的训练流程，现在随机生成一些数据进行模型训练：\n",
    "#### 3.2.1 使用numpy输入数据并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x = np.random.random((1000, 72))\n",
    "train_y = np.random.random((1000, 10))\n",
    "\n",
    "val_x = np.random.random((200, 72))\n",
    "val_y = np.random.random((200, 10))\n",
    "\n",
    "model.fit(train_x, train_y, epochs=10, batch_size=100,\n",
    "          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家的模型是否都跑起来了呢？如果成功的话，可以看到每一个epoch训练的时间、训练集的评价和验证集的评价。\n",
    "\n",
    "细心的同学可能已经发现了，此时随机生成的标签y其实上是无意义的，应该是onehot表示才对。不过此时我们只是初步试验一下模型，所以只要维度正确模型的含义可以先不管~\n",
    "#### 3.2.2 使用tf.data输入数据并训练\n",
    "除了使用numpy输入，也可以通过`tf.data`将训练和验证集拼接后输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 17:57:08.856054  2016 training_utils.py:1436] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 11.5227 - categorical_crossentropy: 11.5227 - val_loss: 11.5489 - val_categorical_crossentropy: 11.5489\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 11.5497 - categorical_crossentropy: 11.5490 - val_loss: 11.5726 - val_categorical_crossentropy: 11.5726\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 11.5627 - categorical_crossentropy: 11.5624 - val_loss: 11.5529 - val_categorical_crossentropy: 11.5529\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.5434 - categorical_crossentropy: 11.5423 - val_loss: 11.5552 - val_categorical_crossentropy: 11.5552\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 11.5096 - categorical_crossentropy: 11.5078 - val_loss: 11.5551 - val_categorical_crossentropy: 11.5551\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.5554 - categorical_crossentropy: 11.5548 - val_loss: 11.5561 - val_categorical_crossentropy: 11.5561\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 11.5493 - categorical_crossentropy: 11.5486 - val_loss: 11.5564 - val_categorical_crossentropy: 11.5564\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 11.5337 - categorical_crossentropy: 11.5326 - val_loss: 11.5564 - val_categorical_crossentropy: 11.5564\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.5512 - categorical_crossentropy: 11.5504 - val_loss: 11.5576 - val_categorical_crossentropy: 11.5576\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.5732 - categorical_crossentropy: 11.5729 - val_loss: 11.5569 - val_categorical_crossentropy: 11.5569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2808b35c308>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "val_dataset = val_dataset.repeat()\n",
    "\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30,\n",
    "          validation_data=val_dataset, validation_steps=3, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `model.fit`参数配置\n",
    "在跑通了第一个模型之后，大家平复一下激动的心情，来看一下训练中的参数：\n",
    "- X：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array\n",
    "- y：标签，numpy array\n",
    "- batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\n",
    "- epochs：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为\"number of\"的意思\n",
    "- steps_per_epoch：整数，含义为每个epoch分割为多少个batch_size\n",
    "- validation_data：形式为（X，y）的tuple，是指定的验证集。\n",
    "- shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。\n",
    "\n",
    "### 3.4 评估与预测\n",
    "现在到了检验模型效果的时候了，可以使用`model.evaluate`和`model.predict`函数进行模型的评价和预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 178us/sample - loss: 11.5061 - categorical_crossentropy: 11.5061\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 11.5240 - categorical_crossentropy: 11.5240\n",
      "[[0.10536939 0.10557511 0.10149916 ... 0.10310996 0.09730556 0.09919092]\n",
      " [0.099891   0.10458206 0.0997462  ... 0.10576174 0.09858138 0.09883048]\n",
      " [0.10536939 0.10557511 0.10149916 ... 0.10310996 0.09730556 0.09919092]\n",
      " ...\n",
      " [0.09775952 0.10422397 0.0995644  ... 0.10562015 0.0998134  0.0984407 ]\n",
      " [0.10536939 0.10557511 0.10149916 ... 0.10310996 0.09730556 0.09919092]\n",
      " [0.1022763  0.10496268 0.09990297 ... 0.10588399 0.09725997 0.09919334]]\n"
     ]
    }
   ],
   "source": [
    "test_x = np.random.random((1000, 72))\n",
    "test_y = np.random.random((1000, 10))\n",
    "model.evaluate(test_x, test_y, batch_size=32)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "test_data = test_data.batch(32).repeat()\n",
    "model.evaluate(test_data, steps=30)\n",
    "# predict\n",
    "result = model.predict(test_x, batch_size=32)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处的batch_size为一次传入的数据数目。如果硬件允许的话，数值越大速度越快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 保存和恢复\n",
    "### 4.1 权重的保存和读取\n",
    "可以通过`model.save_weights`和`model.load_weights`来进行权重的保存和读取，参数为文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../weights/weights')\n",
    "model.load_weights('../weights/weights')\n",
    "model.save_weights('../weights/weights.h5')\n",
    "model.load_weights('../weights/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 保存网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backend': 'tensorflow',\n",
      " 'class_name': 'Sequential',\n",
      " 'config': {'layers': [{'class_name': 'Dense',\n",
      "                        'config': {'activation': 'relu',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'batch_input_shape': [None, 72],\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 32,\n",
      "                                   'use_bias': True}},\n",
      "                       {'class_name': 'Dense',\n",
      "                        'config': {'activation': 'relu',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_1',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 64,\n",
      "                                   'use_bias': True}},\n",
      "                       {'class_name': 'Dense',\n",
      "                        'config': {'activation': 'softmax',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_2',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 10,\n",
      "                                   'use_bias': True}}],\n",
      "            'name': 'sequential'},\n",
      " 'keras_version': '2.2.4-tf'}\n",
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "  layers:\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: relu\n",
      "      activity_regularizer: null\n",
      "      batch_input_shape: !!python/tuple\n",
      "      - null\n",
      "      - 72\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense\n",
      "      trainable: true\n",
      "      units: 32\n",
      "      use_bias: true\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: relu\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_1\n",
      "      trainable: true\n",
      "      units: 64\n",
      "      use_bias: true\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: softmax\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_2\n",
      "      trainable: true\n",
      "      units: 10\n",
      "      use_bias: true\n",
      "  name: sequential\n",
      "keras_version: 2.2.4-tf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "# 序列化成json\n",
    "import json\n",
    "import pprint\n",
    "json_str = model.to_json()\n",
    "pprint.pprint(json.loads(json_str))\n",
    "fresh_model = tf.keras.models.model_from_json(json_str)\n",
    "# 保持为yaml格式  #需要提前安装pyyaml\n",
    "\n",
    "yaml_str = model.to_yaml()\n",
    "print(yaml_str)\n",
    "fresh_model = tf.keras.models.model_from_yaml(yaml_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 保存整个模型\n",
    "可以通过`model.save`和`tf.keras.models.load_model`命令保存和读取整个模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/all_model.h5')\n",
    "model = tf.keras.models.load_model('../models/all_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里我们就完成了全部第一课的学习啦，现在我们已经可以通过`keras.layers`内置的全连接层，搭建我们自己的网络结构啦~大家可以可以找一找身边数据集，have a try！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
