{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test1. 实现mnist手写字体识别（全连接神经网络）\n",
    "上节课我们已经实现了使用tf中的keras构建简单网络，现在我们尝试用简单的全连接网络来解决经典的mnist手写字体识别问题：\n",
    "### 1.1 导入tf、keras和keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 导入并处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 60000\n",
      "<class 'numpy.ndarray'> 10000\n",
      "<class 'numpy.ndarray'> (28, 28)\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(type(x_train), len(x_train))\n",
    "print(type(y_test), len(y_test))\n",
    "print(type(x_train[0]), np.shape(x_train[0]))\n",
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出训练集有60000个样本，测试集有10000个，每个x有28\\*28维特征，y为一维的字体分类。\n",
    "\n",
    "接下来有两个预处理步骤：\n",
    "- 因为使用全连接网络，可以将接下来我们要将x直接拼接为1\\*784维度，并且统一转化为tensorflow常用的float32变量；\n",
    "- x中的值为图像的灰度表示（0~255），可以做一个归一化。(会对结果有影响吗？)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.2        0.62352943 0.99215686 0.62352943 0.19607843\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1882353  0.93333334\n",
      " 0.9882353  0.9882353  0.9882353  0.92941177 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.21176471 0.8901961  0.99215686 0.9882353  0.9372549\n",
      " 0.9137255  0.9882353  0.22352941 0.02352941 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03921569 0.23529412 0.8784314\n",
      " 0.9882353  0.99215686 0.9882353  0.7921569  0.32941177 0.9882353\n",
      " 0.99215686 0.47843137 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6392157  0.9882353  0.9882353  0.9882353  0.99215686\n",
      " 0.9882353  0.9882353  0.3764706  0.7411765  0.99215686 0.654902\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.2        0.93333334\n",
      " 0.99215686 0.99215686 0.74509805 0.44705883 0.99215686 0.89411765\n",
      " 0.18431373 0.30980393 1.         0.65882355 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.1882353  0.93333334 0.9882353  0.9882353  0.7019608\n",
      " 0.04705882 0.29411766 0.4745098  0.08235294 0.         0.\n",
      " 0.99215686 0.9529412  0.19607843 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.64705884\n",
      " 0.99215686 0.9137255  0.8156863  0.32941177 0.         0.\n",
      " 0.         0.         0.         0.         0.99215686 0.9882353\n",
      " 0.64705884 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02745098 0.69803923 0.9882353  0.9411765  0.2784314\n",
      " 0.07450981 0.10980392 0.         0.         0.         0.\n",
      " 0.         0.         0.99215686 0.9882353  0.7647059  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.22352941\n",
      " 0.9882353  0.9882353  0.24705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.99215686 0.9882353  0.7647059  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.7764706  0.99215686 0.74509805\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.99215686\n",
      " 0.76862746 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.29803923 0.9647059  0.9882353  0.4392157  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99215686 0.9882353  0.5803922  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.33333334 0.9882353\n",
      " 0.9019608  0.09803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02745098 0.5294118\n",
      " 0.99215686 0.7294118  0.04705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333334 0.9882353  0.8745098  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02745098 0.5137255  0.9882353  0.88235295 0.2784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333334 0.9882353  0.5686275  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1882353  0.64705884\n",
      " 0.9882353  0.6784314  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3372549  0.99215686\n",
      " 0.88235295 0.         0.         0.         0.         0.\n",
      " 0.         0.44705883 0.93333334 0.99215686 0.63529414 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333334 0.9882353  0.9764706  0.57254905\n",
      " 0.1882353  0.11372549 0.33333334 0.69803923 0.88235295 0.99215686\n",
      " 0.8745098  0.654902   0.21960784 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333334 0.9882353  0.9882353  0.9882353  0.8980392  0.84313726\n",
      " 0.9882353  0.9882353  0.9882353  0.76862746 0.50980395 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.10980392 0.78039217\n",
      " 0.9882353  0.9882353  0.99215686 0.9882353  0.9882353  0.9137255\n",
      " 0.5686275  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09803922 0.5019608  0.9882353\n",
      " 0.99215686 0.9882353  0.5529412  0.14509805 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784).astype('float32')/255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32')/255\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 网络搭建\n",
    "现在我们开始搭建我们的模型：\n",
    "1. 784维的输入\n",
    "2. 32个神经元的全连接层\n",
    "3. 64个神经元的全连接层\n",
    "4. 10个神经元的softmax输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(0.005),\n",
    "             loss = keras.losses.sparse_categorical_crossentropy,\n",
    "             metrics = [keras.losses.sparse_categorical_crossentropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.0465 - sparse_categorical_crossentropy: 0.0465 - val_loss: 0.2374 - val_sparse_categorical_crossentropy: 0.2374\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s 61us/sample - loss: 0.0326 - sparse_categorical_crossentropy: 0.0326 - val_loss: 0.2570 - val_sparse_categorical_crossentropy: 0.2570\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 61us/sample - loss: 0.0292 - sparse_categorical_crossentropy: 0.0292 - val_loss: 0.2482 - val_sparse_categorical_crossentropy: 0.2482\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s 62us/sample - loss: 0.0342 - sparse_categorical_crossentropy: 0.0342 - val_loss: 0.2601 - val_sparse_categorical_crossentropy: 0.2601\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 61us/sample - loss: 0.0318 - sparse_categorical_crossentropy: 0.0318 - val_loss: 0.2652 - val_sparse_categorical_crossentropy: 0.2652\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s 62us/sample - loss: 0.0273 - sparse_categorical_crossentropy: 0.0273 - val_loss: 0.2623 - val_sparse_categorical_crossentropy: 0.2623\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s 61us/sample - loss: 0.0278 - sparse_categorical_crossentropy: 0.0278 - val_loss: 0.2961 - val_sparse_categorical_crossentropy: 0.2961\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s 60us/sample - loss: 0.0238 - sparse_categorical_crossentropy: 0.0238 - val_loss: 0.2831 - val_sparse_categorical_crossentropy: 0.2831\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s 59us/sample - loss: 0.0244 - sparse_categorical_crossentropy: 0.0244 - val_loss: 0.2696 - val_sparse_categorical_crossentropy: 0.2696\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s 59us/sample - loss: 0.0282 - sparse_categorical_crossentropy: 0.0282 - val_loss: 0.2759 - val_sparse_categorical_crossentropy: 0.2759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cfafabb1c8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=100, \n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 在验证集上测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 106us/sample - loss: 0.2632 - sparse_categorical_crossentropy: 0.2632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2631902001183575, 0.26319027]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就完成了简单的全连接神经网络对mnist手写字体识别的分类问题，大家可以根据自己爱好调整策略和超参数，开始成为调参侠的第一步啦~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
